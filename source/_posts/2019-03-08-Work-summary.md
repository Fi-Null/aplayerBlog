---
title: Work summary
date: 2019-03-08 23:32:45
category:
- summary
---
## 订单中心：

### 简介：

负责生成订单，把控整个订单生命周期（支付完成->补全-》下发商家-〉妥投等流程），通知下游系统订单状态变更，订单查询服务。

### 现状：

接手之前，COE频发。每年至少出3-4次事故，给平台造成巨大的经济损失，用户流失。

力保系统必须稳定，我们的订单时效是一小时，高峰期一旦系统出问题造成订单无法生产，在1个小时内是解决不了的，压力很大。

观察以往事故经验，出事故主要集中在Es查询服务这边，查询超时，商家查不到订单，无法生成。

排查Es查询服务，发现Es集群服务器Cpu经常抖动，cpu使用使用率飙高到90%，于是查询原因：（1）Es集群混布，其他系统的es集群也不部署在订单中心查询服务器上，高峰期存在资源抢占，分配不均等行为。（2）Es单集群，存储到家3年的订单，大概10几亿条数据，数据量大，订单的复杂查询（比如多条件聚合查询）多，聚合查询每分钟能达到500k。

### 技术架构升级优化：

（1）单独搭建一套新的Es热集群，集群资源不与任何系统共享，消除混布带来的不稳定。并且热集群只保存5天的数据，大概500万左右。思考我们的业务，其实99%的订单都会在一小时内送出去，热集群保存5天数据用于生产环境的查询完全是没有任何问题。如果商家有查询历史数据的需求，可以继续查原来的冷集群全量数据。

（2）冷热双集群问题不仅解决了之前的查询问题。还解决了之前的单点故障，如果Es热集群挂了，可以立马切换到冷集群。之前数据是双写的，冷集群数据好似全量数据。热双集群实时可以随时切换。

升级订单中心Es单集群架构为冷热双集群架构，不仅解决集群单点故障，还解决订单数据查询和存储瓶颈等问题，使订单中心处理订单处理能力由日均百万级单量升级为千万级。

### 业务升级：

#### 成果：

开发到家配送联盟，使订单的履约率有96.5%提升到99.5%。配送联盟是去年参加<strong>hackthon</strong>的idea，并且深受各位评委的好评，夺得<strong>hackthon</strong>一等奖，并且也获得了去年的公司最佳项目一等奖。 

#### 背景（why）：

观察数据，发现到家配送履约率只有96.5%，100单就有4单没有配送。分析其原因：配送只有达达一方，有些地方达达运力覆盖不足，导致无人配送。于是萌生出到家配送联盟的idea。订单抛给达达，达达在X（可配置）分钟内未接单，同时抛单给美团、顺丰、点我达、闪送。

####  优化

hackthon获奖后，一人独自开发完整的到家配送联盟系统，一周之内对接完所有的三方配送并且上线。并且之后对业务做了一些优化。（1）高价值的数码订单交给顺丰、闪送。（2）按时段抛单，每个时段的抛单时间不对。（医药订单的夜间配送直接抛给美团。）

#### 后续对配送联盟的下一步思考：

抛单策略比较粗暴。需要一个抛单策略算法，由算法综合各个因需（ 各个配送运力情况，配送完成率，每单成本【0-3公里 美团便宜；3公里以上，点我达便宜】等）评估，最终做到最适合抛单的配送。

## 商家中心   

### 简介：

对接开放平台  对接商家，提供商家各种操作订单的能力（接单-》打印-》捡货-〉配送-》妥投，异常流程）。

### 技术优化：

#### 现象：

Young Gc慢，一次Young Gc达到了夸张的4000ms。造成很多对外提供的的接口响应时间超时，所有接口的 tp 999都超过了4s，对商家的生产造成了很大的影响。

#### 分析：

常规方法：打印Gc日志，并分析Gc日志。但是观察Gc日志，Young gc的详细过程【1、级联扫描Young区根对象。2、扫描卡表时间。3、扫描根直接在old区的根对象】并没有，无法查明具体原因。通过查阅资料和观察，发现young gc慢的机器，长时间【5天】不执行full gc。回忆young gc过程，发现耗时应该是出现在扫描卡表这块。

<strong>卡表：</strong>

执行young gc的时候，如果有老年代的对象引用年轻代的对象，年轻代的对象不应该被标记清理。然而young gc是不可能是扫描整个old区，这样young gc时间大幅增加。并且old区引用年轻代情况不到1%。于是引入卡表，将old区分成许多大小相等的卡表，如果卡表里有对象引用年轻代对象，卡表标记为脏。下次young gc的时候只用扫描卡表，避免扫描整个old区。

扫描卡表的时间，ali jdk有参数可以打印时间，普通的jdk没有。

长时间不执行full gc，导致状态为脏的卡表越来越多，扫描越来越耗时。

#### 解决：

（1）调整Jdk参数，调小old区，调小晋升年龄，适当加大young区，适当增加gc线程【减少young gc次数，提高系统吞吐量】。

（2）每天夜里12点，业务量很小的时候，在服务器起一个定时任务手动执行gc。

2者一起使用。gc参数随着系统的访问量而调整，在上最后一道保险是手动执行gc。

#### 结果：

 最终Ygc 时间从4000ms到50ms。

### 业务升级

商家中心做了很多事为商家赋能，提高商家的生产力。主要的2项：长链接推送中台服务，合流墙项目。

#### 长链接推送服务

##### 问题：

商家经常抱怨订单来了，他们不知道。自驱开发基于netty长链接的高性能推送中台服务解决漏订单问题。之前的声音提醒是用Ajax 实时轮询查询后端es，门店不管有没有单，都会定时轮讯查询Es，查询条件很复杂，并且是很耗费性能的聚合查询，造成Es集群压力很大，常常影响到其他查询服务。在经过技术调研和预演，最终使用Netty开发出基于websocket协议的长链接推送中台服务。目前长链接服务主要用于Web端的自动打印和声音提醒。具体的过程【拿声音提醒为例】是这样：长链接服务接受Mq【比如订单中心下发商家订单Mq】，向前端发送消息，前端接到消息后播放声音提醒语音，同时回传接收成功消息。如果没有回传，长链接服务器会再次发出消息，直至前端回传成功。这样保证了消息的触达。

效果很好，其他团队也在接入使用。长链接数量上升，在100万左右。单机会出现瓶颈，单点故障等问题。

架构升级：分布式改造中台推送服务遇到的困难：

（1）长链接自动断开。

起初以为是长链接服务代码的问题。经过许多次的测试和发现，发现前端链接长链接服务如果是通过Ip直联的方式不存在断开问题，故排除长链接服务问题。为了实现负载均衡前端通过京东网关链接长链接服务器， 而网关有一个心跳检查机制，每隔5分钟检查客户端和服务端的通讯状态，如果5分钟内没有通讯，则会断开长链接服务。

<strong>解决：</strong>

​	自写前端js脚本，实现每隔3分钟，向后端发送心跳信息，长链接服务器在应答，这样便实现一次通讯。成功欺骗过京东网关，实现长链接永不断开。并且及时长链接被断开，也实现了断开也会重连。

（2）路由问题。长链接中台服务后端有多台机器，推送的时候是不知道具体建立的长链接通道在那台服务器。

解决：

1、扩展长链接服务，由于使用netty，很容易支持http。方案是A、B任意一台服务器接收到mq消息，向所有的长链接服务器发送http服务，把消息推给所有服务器，最总只会有一台服务器推送成功，其他服务器没有推送通道

2、mq广播

3、引入redis缓存记录路由关系。key：门店编号，value：ip地址列表

4、引入注册中心，每次链接都向注册中心注册，下次接到mq后，从注册中心获取长链接通道所在的机器ip

最终选择1，实现简单，还能对外对接方便，并且内网的http调用开销很小，部署机器3台不多，多2次http而已。

#### 合流墙项目

##### 问题：

目前到家对接了全国基本所有的大型超市。比如永辉，沃尔玛等。但是这个大超市有一个特点门店面积很大，达到几万平米。订单时效是1小时，顾客买的东西遍布于几万平米的各个地方，于是如何快速的捡货成为了发展瓶颈。前期设置前置仓解决了大部分问题。前置仓就是超市特定为到家设置200平米的捡货区域，这个区域包含最热门的sku商品。但是这样做提供的服务是有损的，不能买到超市的所有东西，把超市的所有sku都搬到线上去。于是合流墙便产生了。

合流墙项目是我带领3人小团队突击开发在一个月内完成上线的。从0到1搭建了整个项目的架构，并且实现了所有核心流程的代码。

合流墙的思想是：将订单按商品分类拆分成多个捡货单，每个分类安排几个捡货员，这样实现了并行捡货，最后每个捡货任务捡完后合并打包送出。

合流墙上线后大大提高了商家捡货能力，捡货时间由30分钟下降为15分钟左右。让商家原来每天只能生产2000订单，可以攀升到5000单。

## 整体思考

订单业务越来越复杂，定制化需求越来越多，代码复杂度越来越高，最终导致难以维护。

目前订单中心对接了线下赋能业务，京东订单中心业务，自提业务。 于是大中台概念

要做的事：

（1）订单中台只负责订单的基本操作【定义订单生产的各个节点：支付完成-> 暂停补全信息-〉下发商家-> 等待出库->配送中-〉妥投完成。订单状态的变更通知，查询订单基础信息】，并且每个操作都是原子性的，不牵扯任何业务逻辑。单独提供sdk给其他业务系统，各自定制订单的各个流程【比如线下自提订单，订单流程：支付完成-》妥投】。各个流程可以供其他业务随意定义。这样订单中心可以应对各种复杂业务场景，实现自配置。

技术上：

实现高可用，可伸缩，可熔断降级。 rpc框架提供远程调用服务，jmq提供削峰，异步处理，消息通知。

搭建异地多数据中心，实现数据高可用。 搭建es冷热双集群，实现查询服务的高可用。

考虑新技术TiDb，完全兼容mysql协议，不用分库分表【扩容迁移数据问题，多表join问题，聚合查询问题】，可以无限水平扩容。



问问题：

公司对新入职的员工的培养机制是什么样的呢

新人怎么快速融入团队。

核算是一个什么部门，可以介绍下吗，我的理解就是计费？

团队现在面临的最大挑战是什么

针对我的面试表现，你觉得我还有那些地方可以提高？



薪资。3月加薪加7月升值，期望35k，至少32k

看中长远发展，看中大平台。全球领先的平台，服务于几十亿用户，做更有挑战的事。和更优秀的人共事，成长得更快。

缺点：容易紧张。表达能力一般，需要提高。

优点：吃苦耐劳，肯专研，技术有追求。

杭州。北方不习惯。
